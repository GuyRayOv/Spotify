# ğŸ¯ Machine Learning Pipeline Project

This repository contains a complete, modular **machine learning pipeline**, developed using Kaggle's Spotify dataset.  
The original dataset documentation by Kaggles is available at: `{REPOSITORY_PATH}/data/README.md`

---

## ğŸ“˜ Project Overview

The project implements all major stages of a **supervised machine learning workflow**, including:

1. **Data Preparation** â€“ loading, cleaning, and structuring the raw dataset.  
2. **Exploratory Data Analysis (EDA)** â€“ statistical summaries, visualization, and feature insights.  
3. **Data Cleansing** â€“ outlier detection and imputation of missing values.  
4. **Feature Engineering & Selection** â€“ feature transformation, encoding, and importance ranking.  
5. **Model Selection & Hyperparameter Optimization** â€“ model evaluation and tuning using Grid Search.

Each stage is implemented in a dedicated **Jupyter notebook (.ipynb)** to ensure modularity, clarity, and reproducibility.

---

ğŸ§© Project Structure
```
{REPOSITORY_PATH}/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ README.md                # Kaggle dataset description
â”‚   â””â”€â”€ Spotify Dataset.pptx     # Project presentation with visualizations
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_spotify_dataprep.ipynb
â”‚   â”œâ”€â”€ 2_spotify_eda.ipynb
â”‚   â”œâ”€â”€ 3_spotify_datacleansing.ipynb
â”‚   â”œâ”€â”€ 4_spotify_fe.ipynb
â”‚   â””â”€â”€ 5_spotify_models.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.json              # Global project configuration file
â”‚   â”œâ”€â”€ utils.py                 # Helper functions (shared across notebooks)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ output/                      # Logs, results, and generated files
â”œâ”€â”€ pickles/                     # Serialized models and data
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # Project documentation (this file)
â””â”€â”€ .gitignore                   # Ignored files and folders
```

---

## âš™ï¸ Notebook Workflow

Notebooks are **cascading each other** â€” Notebook *N* executes notebook *N-1* first and loads its serialized output (`N-1.pkl`). 
Notebook '(REPOSITORY_PATH}/notebooks/1_spotify_dataprep.ipynb' starts its execution by downloading the Spotify dataset from Kaggle's webseite.
Each stage can also be run independently. This behavior is controlled via the `chain_df` flag in `config.json`.

A centralized configuration file â€”  `{REPOSITORY_PATH}/src/config.json` â€” defines all global parameters, including:
- File paths
- Execution order  
- Algorithms for outlier detection, imputation, and modeling  
- Pipeline control flags (e.g., `split_df`, `chain_notebooks`)

---

## ğŸ§  Data Leakage Prevention

To ensure robust evaluation and prevent data leakage, the Kaggle dataset is split into two independent subsets:

- **Training DataFrame:** `df`  
- **Testing DataFrame:** `df_test`

Both are processed independently through the pipeline. This behavior is controlled via the `split_df` flag in `config.json`.

---

## ğŸš€ How to Run

1. **Clone** this repository to your local machine.  
2. Create `.env` file in the root directory of the Runtime, e.g. `/contect/.env`
3. In `.env` define `PROJECT_PATH` to match your local copy. e.g. `PROJECT_PATH=/content/drive/MyDrive/Projects/GitHub/Spotify/`
4. In `{REPOSITORY_PATH}/src/config.json`, If `chain_notebooks : 1`, the entire pipline will be executed. Or `0` for a run of a sinlge notebook.
5. Open `{REPOSITORY_PATH}/notebooks/5_spotify_models.ipynb`, for an execution of the entire pipline. Or any notebook for a partial run.
7. Click **Run All**.

---

## ğŸ“Š Project Deliverables

- **Jupyter Notebooks** â€“ one per ML pipeline stage â†’ `{REPOSITORY_PATH}/notebooks/`  
- **Configuration File** â€“ centralized global parameters â†’ `{REPOSITORY_PATH}/src/config.json`  
- **Presentation Deck** â€“ key insights and visualizations â†’ `{REPOSITORY_PATH}/data/Spotify Dataset.pptx`

---
