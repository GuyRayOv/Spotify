{"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"KTEI-wCzrAVM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"io9eEHq7hZKq"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","import warnings\n","from google.colab import files\n","import pickle\n","import os\n","\n","!pip install scipy\n","import scipy.stats as stats\n","\n","!pip install papermill\n","!pip install nbconvert\n","!pip install nbformat\n","!pip install IPython\n","\n","import papermill as pm\n","import nbformat\n","from nbconvert import HTMLExporter\n","from IPython.display import HTML, display\n","\n","import json\n","from google.colab import drive\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","import warnings\n","from google.colab import files\n","import pickle\n","import os\n","from scipy import stats\n","from scipy.stats import zscore\n","\n","!pip install missingno\n","import missingno as msno\n","\n","!pip install fancyimpute\n","import fancyimpute\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"markdown","source":["# Mount drive, Load config"],"metadata":{"id":"24gu38NjrFgK"}},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","\n","# Find and load the .env file from the current or parent directories\n","load_dotenv()\n","\n","# Access the environment variable using os.getenv()\n","PROJECT_PATH = os.getenv('PROJECT_PATH')\n","CONFIG_FILE = f\"{PROJECT_PATH}config.json\""],"metadata":{"id":"Gch0QCmZIG5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","\n","with open(CONFIG_FILE, 'r') as f:\n","    project_config = json.load(f)\n","    project_config.pop('_comment', None)\n","    project_config.pop('_note', None)\n","    f.close()"],"metadata":{"id":"jsWS0uXZwymE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tP96XFBs88Jq"},"outputs":[],"source":["np.random.seed(31071967)"]},{"cell_type":"markdown","source":["# Run project notebook N-1"],"metadata":{"id":"yNfqnT_VrKI2"}},{"cell_type":"code","source":["if project_config['chain_notebooks'] == '1':\n","\n","  input_file = f\"{PROJECT_PATH}{project_config['notebooks_directory']}{project_config['notebook2']}\"\n","  output_file = f\"{PROJECT_PATH}{project_config['output_directory']}{project_config['output2']}\"\n","\n","  # --- Execute the proviuse notebook with parameters ---\n","  pm.execute_notebook(\n","      input_path = input_file,\n","      output_path = output_file,\n","      log_output=False,  # don't print logs while running\n","      progress_bar=True\n","  )\n","\n","  # --- Convert the executed notebook to HTML ---\n","  nb = nbformat.read(output_file, as_version=4)\n","  html_exporter = HTMLExporter()\n","  html_exporter.template_name = \"lab\"  # modern look; alternatives: 'classic', 'basic'\n","  body, _ = html_exporter.from_notebook_node(nb)\n","\n","  # --- Display the HTML result inline ---\n","  display(HTML(body))"],"metadata":{"id":"YY6gc_3UZbUo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xHrmpZfYh1nd"},"source":["# Load pickel"]},{"cell_type":"code","source":["pickle_file      = PROJECT_PATH + project_config['pickles_directory'] + project_config['pickle1']\n","test_pickle_file = PROJECT_PATH + project_config['pickles_directory'] + project_config['pickle1_test']"],"metadata":{"id":"qbNQzPjJPel9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaGg_6Cuh5AA"},"outputs":[],"source":["# Read pickle into DataFrame\n","df = pd.read_pickle(pickle_file)\n","display(df.head(1))\n","\n","if project_config['split_df'] == '1':\n","  df_test = pd.read_pickle(test_pickle_file)\n","  display(df_test.tail(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTGf0x0TSmn-"},"outputs":[],"source":["# types of cols\n","small_cat_cols     = ['mode_mean_popularity','key_mean_popularity', 'playlist_genre_grouped_mean_popularity','release_decade_mean_popularity','release_month_mean_popularity']\n","small_cat_symboles = ['mode','key','playlist_genre_grouped','release_decade','release_month', 'release_year']\n","large_cat_cols     = ['track_artist_mean_popularity','track_album_id_mean_popularity','playlist_id_mean_popularity']\n","cont_cols          = ['acousticness', 'danceability','duration_ms','energy', 'liveness', 'loudness', 'speechiness', 'tempo','valence']\n","y_col              = 'track_popularity'\n","x_cols             = large_cat_cols + small_cat_cols + cont_cols\n","MERGE_ON_COL       = 'track_id'"]},{"cell_type":"markdown","metadata":{"id":"8dt8-ie2uIYy"},"source":["# Outliers detection\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnoeF8HH5bbU"},"outputs":[],"source":["cols_to_check_for_outliers = cont_cols"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnUUXHXh8peP"},"outputs":[],"source":["# keep a copy for before&after plots\n","df_with_outliers = df.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILP3Isdr-5df"},"outputs":[],"source":["if project_config['outliers_alg'] == 'IQR':\n","\n","  multiplier = 1.5\n","  total_outliers_repleced = 0\n","\n","  for col in cols_to_check_for_outliers:\n","    Q1 = df[col].quantile(0.25)\n","    Q3 = df[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - multiplier * IQR\n","    upper = Q3 + multiplier * IQR\n","    mask = (df[col] < lower) | (df[col] > upper)\n","\n","    # Replace outliers\n","    df.loc[mask, col] = np.nan\n","    total_outliers_repleced += mask.sum()\n","\n","    print(f\"{col}: replaced {mask.sum()} outliers\")\n","\n","  print(f\"\\nA total of {total_outliers_repleced} cells replaced with NaN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34EXZrXuKBRz"},"outputs":[],"source":["def identify_outliers_with_mad(df, cols_to_check_for_outliers, impute_with='NaN'):\n","\n","  total_outliers_repleced = 0\n","\n","  for col in cols_to_check_for_outliers:\n","\n","  #identify outliers using MAD\n","    median = df[col].median()\n","    mad = np.median(np.abs(df[col] - median))\n","    modified_z = 0.6745 * (df[col] - median) / mad\n","\n","    mask = np.abs(modified_z) > 3\n","\n","  # replace with NaN\n","    df.loc[mask, col] = np.nan\n","    total_outliers_repleced += mask.sum()\n","    print(f\"{col}: replaced {mask.sum()} outliers, using a modified_z threshold of {3}\")\n","\n","  print(f\"\\nA total of {total_outliers_repleced} cells replaced with NaN\")\n","\n","##########################################################################\n","if project_config['outliers_alg'] == 'MAD':\n","  identify_outliers_with_mad(df, cols_to_check_for_outliers, impute_with='NaN')\n","\n","  if project_config['split_df'] == '1':\n","    identify_outliers_with_mad(df_test, cols_to_check_for_outliers, impute_with='NaN')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hI3_7UlY6Jtj"},"outputs":[],"source":["if project_config['outliers_alg'] == 'Z_SCORE':\n","\n","  # compute z-scores\n","  Z_score = np.abs(stats.zscore(df[cols_to_check_for_outliers], nan_policy='omit'))\n","\n","  # create a mask of True where Z > 3\n","  mask = Z_score > outliers_detection_threshold\n","\n","  # count replaced cells per column\n","  replaced_counts = mask.sum(axis=0)\n","\n","  # pretty print with column names\n","  print(f\"{replaced_counts.sum()} cells replaced with NaN (Z > 3):\")\n","  for col, count in zip(cols_to_check_for_outliers, replaced_counts):\n","      print(f\"{col}: {count}\")\n","\n","  # actually replace with NaN\n","  df[cols_to_check_for_outliers] = df[cols_to_check_for_outliers].mask(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAW19614C5jC"},"outputs":[],"source":["def identify_outliers_with_iso(df, cols_to_check_for_outliers, impute_with='NaN'):\n","\n","  # Fit Isolation Forest\n","  iso = IsolationForest(contamination=0.05, random_state=42)\n","  df['outlier_iso'] = iso.fit_predict(df[cols_to_check_for_outliers])\n","\n","  # Label outliers\n","  outliers_iso = df[df['outlier_iso'] == -1]\n","  print(f\"IsolationForest found {len(outliers_iso)} outliers\")\n","\n","  # Replace outlier values with NaN\n","  for col in cols_to_check_for_outliers:\n","    mask = df['outlier_iso'] == -1 # -1 if 'iso' in outlier_col or 'lof' in outlier_col else df[outlier_col] == 1\n","    df.loc[mask, col] = np.nan\n","\n","  df.drop('outlier_iso', axis=1, inplace=True)\n","\n","\n","###############################################################################\n","if project_config['outliers_alg'] == 'ISO':\n","\n","  from sklearn.ensemble import IsolationForest\n","\n","  identify_outliers_with_iso(df, cols_to_check_for_outliers, impute_with='NaN')\n","\n","  if project_config['split_df'] == '1':\n","    identify_outliers_with_iso(df_test, cols_to_check_for_outliers, impute_with='NaN')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gugs7v0oDiSm"},"outputs":[],"source":["def identify_outliers_with_lof(df, cols_to_check_for_outliers, impute_with='NaN'):\n","\n","  num_cols = df.select_dtypes(include='number').columns\n","\n","  lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n","  df['outlier_lof'] = lof.fit_predict(df[cols_to_check_for_outliers])\n","\n","  # Label outliers\n","  outliers_lof = df[df['outlier_lof'] == -1]\n","  print(f\"LOF found {len(outliers_lof)} outliers. Inputing with {impute_with}\")\n","\n","   # Replace outlier values with NaN\n","  for col in cols_to_check_for_outliers:\n","    mask = df['outlier_lof'] == -1 # -1 if 'iso' in outlier_col or 'lof' in outlier_col else df[outlier_col] == 1\n","    df.loc[mask, col] = np.nan\n","\n","  df.drop('outlier_lof', axis=1, inplace=True)\n","\n","\n","##################################################################################\n","if project_config['outliers_alg'] == 'LOF':\n","\n","  from sklearn.neighbors import LocalOutlierFactor\n","\n","  identify_outliers_with_lof(df, cols_to_check_for_outliers, impute_with='NaN')\n","\n","  if project_config['split_df'] == '1':\n","    identify_outliers_with_lof(df_test, cols_to_check_for_outliers, impute_with='NaN')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssFAbVj-D7Z_"},"outputs":[],"source":["def identify_outliers_with_dbscan(df, cols_to_check_for_outliers, impute_with='NaN'):\n","\n","  # Scale features (important for DBSCAN)\n","  X_scaled = StandardScaler().fit_transform(df[cols_to_check_for_outliers])\n","\n","  db = DBSCAN(eps=1.5, min_samples=10)\n","  labels = db.fit_predict(X_scaled)\n","\n","  df['outlier_dbscan'] = (labels == -1).astype(int)\n","\n","  outliers_dbscan = df[df['outlier_dbscan'] == 1]\n","  print(f\"DBSCAN found {len(outliers_dbscan)} outliers\")\n","\n","  # Replace outlier values with NaN\n","  for col in cols_to_check_for_outliers:\n","    mask = df['outlier_dbscan'] == 1 #-1 if 'iso' in outlier_col or 'lof' in outlier_col else df[outlier_col] == 1\n","    df.loc[mask, col] = np.nan\n","\n","  df.drop('outlier_dbscan', axis=1, inplace=True)\n","\n","################################################################################\n","if project_config['outliers_alg'] == 'DBSCAN':\n","\n","  from sklearn.cluster import DBSCAN\n","  from sklearn.preprocessing import StandardScaler\n","\n","  identify_outliers_with_dbscan(df, cols_to_check_for_outliers, impute_with='NaN')\n","\n","  if project_config['split_df'] == '1':\n","    identify_outliers_with_dbscan(df_test, cols_to_check_for_outliers, impute_with='NaN')"]},{"cell_type":"markdown","metadata":{"id":"LzF_AhKKYjlY"},"source":["# Imputate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik86eP3RBR2O"},"outputs":[],"source":["imputer = None\n","outliers_replacement = 'NaN'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICVPPDwZ7xjt"},"outputs":[],"source":["df_with_nulls = df.copy() # keep for visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gehkei_jsDau"},"outputs":[],"source":["if project_config['imputer_alg'] == 'MICE':\n","\n","  from sklearn.experimental import enable_iterative_imputer  # noqa\n","  from fancyimpute import IterativeImputer  # MICE-like\n","\n","  imputer = IterativeImputer(max_iter=15)\n","  df[x_cols] = imputer.fit_transform(df[x_cols])\n","\n","if project_config['split_df'] == '1':\n","  imputer = IterativeImputer(max_iter=15)\n","  df_test[x_cols] = imputer.fit_transform(df_test[x_cols])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvChtZMm7WYV"},"outputs":[],"source":["if project_config['imputer_alg'] == 'KNN':\n","\n","  from fancyimpute import KNN\n","  imputer = KNN(k=5)\n","  df[cols_to_check_for_outliers] = imputer.fit_transform(df[cols_to_check_for_outliers])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwtQ-D5EiAIx"},"outputs":[],"source":["# Find the cells that were NaN before\n","missing_mask = df_with_nulls.isna()\n","\n","# Compare old (NaN) vs new (imputed) values\n","for col in x_cols:\n","    replaced_values = df.loc[missing_mask[col], col]\n","    if not replaced_values.empty:\n","        print(f\"Column: {col} impute {len(replaced_values)+1} NaN cells with values\")\n","        #for idx, new_val in replaced_values.items():\n","        #    print(f\"Row {idx}: filled with {new_val:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"10q7EbEC9Zjc"},"source":["# Visualize the result of cleansing"]},{"cell_type":"markdown","metadata":{"id":"E3JkkdnbmLvo"},"source":["## Using ANOVA to validate distribution of each col along the cleansing process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6S3mUWFCeAt4"},"outputs":[],"source":["results = []\n","\n","for col in cols_to_check_for_outliers:\n","\n","    groups = [\n","        df_with_outliers[col].dropna(),\n","        df_with_nulls[col].dropna(),\n","        df[col].dropna()\n","    ]\n","\n","    # Check data validity before running ANOVA\n","    group_lengths = [len(g) for g in groups]\n","    unique_counts = [g.nunique() for g in groups]\n","\n","    # Detect common problems\n","    if any(l < 2 for l in group_lengths):\n","        reason = \"Too few samples in at least one group\"\n","        f_stat, p_val = np.nan, np.nan\n","    elif all(u == 1 for u in unique_counts):\n","        reason = \"All groups constant (no variance)\"\n","        f_stat, p_val = np.nan, np.nan\n","    else:\n","        try:\n","            f_stat, p_val = stats.f_oneway(*groups)\n","            reason = (\n","                \"Valid test\" if np.isfinite(p_val)\n","                else \"Invalid numeric result (possibly zero variance)\"\n","            )\n","        except Exception as e:\n","            f_stat, p_val, reason = np.nan, np.nan, f\"Error: {e}\"\n","\n","    results.append({\n","        \"column along the process\": col,\n","        \"F_statistic\": f_stat,\n","        \"p_value\": p_val,\n","        \"significant\": (p_val < 0.05) if np.isfinite(p_val) else False,\n","        \"reason\": reason\n","    })\n","\n","anova_diagnostic = pd.DataFrame(results)\n","anova_diagnostic = anova_diagnostic.sort_values(\"p_value\", na_position=\"last\")\n","\n","pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n","display(anova_diagnostic)"]},{"cell_type":"markdown","metadata":{"id":"0pDtyvDBmZJ9"},"source":["## And Scatter plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3Wb6o91CI48"},"outputs":[],"source":["## scatterplots\n","plt.figure(figsize=(20,200))\n","for i, col in enumerate(df[cols_to_check_for_outliers], start=1):\n","  ax = plt.subplot(60, 3, i)\n","  sb.scatterplot(x=df_with_outliers.index, y=df_with_outliers[col], ax=ax, label='with outliers')\n","  sb.scatterplot(x=df_with_nulls.index, y=df_with_nulls[col], ax=ax, label=f\"without outliers, {project_config['outliers_alg']}\")\n","  sb.scatterplot(x=df.index, y=df[col], ax=ax, label=f\"after impute, {project_config['imputer_alg']}\")\n","  plt.subplots_adjust(hspace = 0.7)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sbDQSyrQH9TO"},"source":["# Much better now ✌"]},{"cell_type":"markdown","metadata":{"id":"wou9mbgjaXhi"},"source":["#  Pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcaTZ1859XTf"},"outputs":[],"source":["from google.colab import files\n","import os\n","\n","with open(f\"{PROJECT_PATH}{project_config['pickles_directory']}{project_config['pickle3']}\", 'wb') as f:\n","  pickle.dump(df, f)\n","  f.close()\n","\n","if project_config['split_df'] == '1':\n","  with open(f\"{PROJECT_PATH}{project_config['pickles_directory']}{project_config['pickle3_test']}\", 'wb') as f:\n","    pickle.dump(df_test, f)\n","    f.close()"]},{"cell_type":"markdown","metadata":{"id":"5lsJLcU2keir"},"source":["# Great 😀"]}],"metadata":{"colab":{"provenance":[{"file_id":"18vcfCkg7jsW8f5UYU7g5ZPX07F83A-3w","timestamp":1760167336814}],"authorship_tag":"ABX9TyMr/Vk1Dj8Ez6VhpYWRgkN5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}