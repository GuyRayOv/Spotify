{"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"vy7LcUPlsK0X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbWvCAKmUQD6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","from google.colab import files\n","import os\n","import warnings\n","import pickle\n","import json\n","from google.colab import drive\n","\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"markdown","source":["# Mount drive, Load config"],"metadata":{"id":"zdP_xX80sN98"}},{"cell_type":"code","source":["PROJECT_PATH = \"/content/drive/MyDrive/Projects/GitHub/Spotify/\"\n","CONFIG_FILE = f\"{PROJECT_PATH}src/config.json\""],"metadata":{"id":"Ukv5nQ_7FE9O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_FILE   = \"/content/spotify_data/spotify_songs.csv\""],"metadata":{"id":"dRr_Myb7Oad8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","\n","with open(CONFIG_FILE, 'r') as f:\n","    project_config = json.load(f)\n","    project_config.pop('_comment', None)\n","    project_config.pop('_note', None)\n","    f.close()"],"metadata":{"id":"jsWS0uXZwymE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(31071967)"],"metadata":{"id":"8ImyD7xI-LDZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3b2TInfHPW_"},"source":["#[Download dataset from Kaggle](https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs?select=spotify_songs.csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7z78FIoUOS8"},"outputs":[],"source":["#kaggle_api_key = '{\"username\":\"guyrayov\",\"key\":\"54ae946c7dc7d70618ebf49de4b74dd4\"}'\n","\n","# STEP 1: checking if the dataset exist\n","import os\n","if not os.path.exists(DATA_FILE):\n","\n","  print(\"dowloading Spotify dataset from kaggle....\")\n","\n","  # STEP 2: create API key file: ~/.kaggle/kaggle.json\n","  !pip install kaggle --quiet\n","  !mkdir -p ~/.kaggle\n","  !echo '{\"username\":\"guyrayov\",\"key\":\"54ae946c7dc7d70618ebf49de4b74dd4\"}' > ~/.kaggle/kaggle.json\n","  !chmod 600 ~/.kaggle/kaggle.json\n","\n","  # STEP 3: Download the dataset from Kaggle\n","  !kaggle datasets download -d joebeachcapital/30000-spotify-songs\n","\n","  # STEP 4: Unzip the dataset\n","  !unzip -o 30000-spotify-songs.zip -d spotify_data\n","\n","  # STEP 5: Check what files are inside. remove the ZIP file\n","  !ls spotify_data\n","\n","  # STEP 6: remove the ZIP file\n","  !rm -rf 30000-spotify-songs.zip\n","\n","else: print(\"Spotify dataset already exists.\")"]},{"cell_type":"markdown","metadata":{"id":"ndergfMCVUU2"},"source":["#Upload dataset with Pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dS5smbVvVIFP"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv(DATA_FILE)\n","display(df.head(1)), display(df.tail(1))"]},{"cell_type":"code","source":["# # appearnatly, five tracks appaers twice (different track_ids), each time in a different playlist. Even popularity is not identical.\n","# # decition: keep both copies\n","# duplicates = (df.groupby(['track_name', 'track_artist', 'track_album_id']).filter(lambda x: x['track_id'].nunique() > 1))\n","# duplicates = duplicates.sort_values(['track_artist', 'track_name', 'track_album_id'])"],"metadata":{"id":"0ccOT1t6DM6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBTQ-sneY2P7"},"outputs":[],"source":["unique_id_cols = ['track_name']\n","dup_cols       = ['track_album_name', 'playlist_name','playlist_genre','instrumentalness']\n","large_cat_cols = ['track_artist', 'track_album_id', 'playlist_id']\n","small_cat_cols = ['mode','key', 'playlist_subgenre']\n","cont_cols      = ['acousticness', 'danceability','duration_ms','energy', 'liveness', 'loudness', 'speechiness', 'tempo','valence']\n","date_cols      = ['release_date']\n","cat_cols       = large_cat_cols + small_cat_cols\n","y_col          = 'track_popularity'\n","MERGE_ON_COL   = 'track_id' # if we'll need to merge pickels later"]},{"cell_type":"markdown","metadata":{"id":"oQQ4OL_-Me_c"},"source":["#Flatfile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBQvB6-NMcB6"},"outputs":[],"source":["def write_flat_file(df, filename):\n","  writer = pd.ExcelWriter(filename, engine=\"openpyxl\")\n","\n","  df.head().to_excel(writer, sheet_name='head')\n","  df.tail().to_excel(writer, sheet_name='tail')\n","  df.describe().to_excel(writer, sheet_name='describe')\n","  df.dtypes.to_excel(writer, sheet_name='data_type')\n","  df.select_dtypes(include=np.number).max().to_excel(writer, sheet_name='max - numeric columns')\n","  df.select_dtypes(include=np.number).min().to_excel(writer, sheet_name='min - numeric columns')\n","  df.isnull().sum(axis=0).to_excel(writer, sheet_name='NA')\n","  df.nunique().to_excel(writer, sheet_name='unique')\n","\n","  writer.close()\n","  print(f\"Flat file: {filename}\")\n","\n","flat_file = \"spotify_flatfile.xlsx\"\n","write_flat_file(df, f\"{project_config['project_path']}{project_config['pickles_directory']}{flat_file}\")"]},{"cell_type":"code","source":["if project_config['split_df'] == '1':\n","  from sklearn.model_selection import train_test_split\n","\n","  # split to avoid data leakage\n","  train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n","\n","  # keep the name df for code compliaency\n","  df = train_df\n","  del train_df\n","\n","  # just a mark\n","  df['set']  = 'train'\n","  test_df['set']  = 'test'\n","\n","else:\n","  test_df = None"],"metadata":{"id":"ui-Suadua6AR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDhm0eKM7Ugc"},"outputs":[],"source":["def pickle_col(df, col='all', drop_col=False, include_merge_ID=True, pickle_name=\"\"):\n","\n","  import pickle\n","\n","  if pickle_name == \"\":\n","    file_name = f\"{project_config['project_path']}{project_config['pickles_directory']}{col}.pkl\"\n","  else:\n","    file_name = f\"{project_config['project_path']}{project_config['pickles_directory']}{pickle_name}.pkl\"\n","\n","  with open(file_name, 'wb') as f:\n","\n","    if col =='all':\n","      pickle.dump(df, f)\n","\n","    elif col in df.columns: # in case we aready droped the col before\n","\n","      # track_id for a later merge, if we need.\n","      #and y_col so can can invetigate the pickel later indepandantly from the main df\n","      pickle.dump(df[[MERGE_ON_COL, col, y_col]], f)\n","\n","    f.close()\n","\n","    if drop_col == True:\n","      df.drop(col, axis=1, inplace=True, errors='ignore')\n","\n","  if project_config['split_df'] == '1':\n","    with open(file_name+\".test.pkl\", 'wb') as f:\n","\n","      if col =='all':\n","        pickle.dump(test_df, f)\n","\n","      elif col in test_df.columns: # in case we aready droped the col before\n","        pickle.dump(test_df[[MERGE_ON_COL, col, y_col]], f)\n","\n","      f.close()\n","\n","    if drop_col == True:\n","      test_df.drop(col, axis=1, inplace=True, errors='ignore')\n","\n","  return df"]},{"cell_type":"code","source":["def apply_cat_mean_as_target_encode(train_df, cat_cols, encode_cols, target_col, test_df=None):\n","\n","  for i in range(len(cat_cols)):\n","    train_means = train_df.groupby(cat_cols[i])[target_col].mean()\n","    train_df[encode_cols[i]] = train_df[cat_cols[i]].map(train_means).astype(float)\n","\n","    if test_df is not None:\n","        test_means = test_df.groupby(cat_cols[i])[target_col].mean()\n","        test_df[encode_cols[i]] = test_df[cat_cols[i]].map(test_means).astype(float)\n","      #test_df[encode_cols[i]] = test_df[cat_cols[i]].map(train_means).astype(float)\n","      #test_df[encode_cols[i]].fillna(train_means, inplace=True)"],"metadata":{"id":"Bt1m7SvUrvXZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ABpi5ChUwOJ"},"source":["# Dates"]},{"cell_type":"code","source":["#track_album_release_date\n","if \"track_album_release_date\" in df.columns: # maybe I'm just rerunnig this cell\n","\n","\n","  df.track_album_release_date = pd.to_datetime(df.track_album_release_date, format='mixed')\n","  df['release_month' ] = df['track_album_release_date'].dt.month.astype(int)\n","  df['release_year'  ] = df['track_album_release_date'].dt.year.astype(int)\n","  df['release_decade'] = ((df['release_year'] // 10) * 10).astype(int)\n","\n","  if project_config['split_df'] == '1':\n","\n","    test_df.track_album_release_date = pd.to_datetime(test_df.track_album_release_date, format='mixed')\n","    test_df['release_month' ] = test_df['track_album_release_date'].dt.month.astype(int)\n","    test_df['release_year'  ] = test_df['track_album_release_date'].dt.year.astype(int)\n","    test_df['release_decade'] = ((test_df['release_year'] // 10) * 10).astype(int)\n","\n","  apply_cat_mean_as_target_encode(\n","      df,\n","      cat_cols=['release_decade', 'release_month','release_year'],\n","      encode_cols=['release_decade_mean_popularity','release_month_mean_popularity','release_year_mean_popularity'],\n","      target_col='track_popularity',\n","      test_df=test_df)\n","\n","  pickle_col(df, col=\"track_album_release_date\", drop_col=True)"],"metadata":{"id":"TWD3fduP0vwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38mgBqol6ZY9"},"outputs":[],"source":["plot_columns = 3\n","plot_rows = 2\n","\n","fig = plt.figure(figsize=(12,12))\n","plt.subplots_adjust(hspace=1.0)\n","sb.set(font_scale=0.8)\n","\n","for plot_counter, col in enumerate(['release_decade','release_year', 'release_month'],start=1):\n","    plt.subplot(plot_rows, plot_columns, plot_counter)\n","    sb.histplot(x=col, data=df)\n","plt.show()\n","\n","fig = plt.figure(figsize=(12,12))\n","plt.subplots_adjust(hspace=1.0)\n","sb.set(font_scale=0.8)\n","\n","for plot_counter, col in enumerate(['release_decade','release_year', 'release_month'],start=1):\n","    plt.subplot(plot_rows, plot_columns, plot_counter)\n","    sb.barplot(x=col, y=f\"{col}_mean_popularity\", data=df)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OHjp-_R-USCt"},"source":["# Categories\n","\n","Uniqe IDs: **drop** to a pickel. Keeping track_id col with each pickel for possible merges later\n","\n","Duplications (album_name, album_id, genre): **drop** to a pickel\n","\n","Lerge cat cols: **replace** with the 'mean' popularity of each category - making it a continuence variabe on\n","\n","Small cat cols: **group** categories based on ANOVA-tests between mean popoularity of each cat. **Replace** with with the 'mean' popularity of each category."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxxTc43d2a1e"},"outputs":[],"source":["# unique ids and duplicaitons: drop\n","for col in unique_id_cols + dup_cols:\n","  df = pickle_col(df, col=col, drop_col=True)"]},{"cell_type":"code","source":["# musical mode\n","df['mode'] = df['mode'].map({0: \"Minor\", 1: \"Major\"}).astype('string')\n","\n","if project_config['split_df'] == '1':\n","  test_df['mode'] = test_df['mode'].map({0: \"Minor\", 1: \"Major\"}).astype('string')"],"metadata":{"id":"nIgnwu0W9hyO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdmXvQ8rr-a9"},"outputs":[],"source":["# 12 categories for musical keys is small enoght to keep\n","# Mapping key-numbers into musical symbols, simpply for a better visualizalizaiton\n","\n","musical_key_dict = {\n","    0: \"C\",\n","    1: \"C♯/D♭\",\n","    2: \"D\",\n","    3: \"D♯/E♭\",\n","    4: \"E\",\n","    5: \"F\",\n","    6: \"F♯/G♭\",\n","    7: \"G\",\n","    8: \"G♯/A♭\",\n","    9: \"A\",\n","    10: \"A♯/B♭\",\n","    11: \"B\"\n","}\n","if df['key'].dtype == 'int64': # or maybe I'm just rerunnig this cell\n","\n","  df['key'] = df['key'].map(musical_key_dict).astype('string')\n","\n","  if project_config['split_df'] == '1':\n","    test_df['key'] = test_df['key'].map(musical_key_dict).astype('string')\n"]},{"cell_type":"code","source":["# using ANOVA for grouping 24 musical genres into 9\n","\n","genre_map = {\n","\n","    'post-teen pop'     : 'Teen Pop',\n","    'permanent wave'    : 'New Wave',\n","\n","    'hip pop'           : 'Mainstream Pop & Hip-Hop',\n","    'hip hop'           : 'Mainstream Pop & Hip-Hop',\n","    'dance pop'         : 'Mainstream Pop & Hip-Hop',\n","    'reggaeton'         : 'Mainstream Pop & Hip-Hop',\n","\n","    'latin pop'         : 'Latin & Urban Trap',\n","    'urban contemporary': 'Latin & Urban Trap',\n","    'trap'              : 'Latin & Urban Trap',\n","\n","    'pop edm'           : 'Electro/Latin Pop Fusion',\n","    'latin hip hop'     : 'Electro/Latin Pop Fusion',\n","    'tropical'          : 'Electro/Latin Pop Fusion',\n","    'electropop'        : 'Electro/Latin Pop Fusion',\n","    'indie poptimism'   : 'Electro/Latin Pop Fusion',\n","\n","    'classic rock'      : 'Classic Rock',\n","    'album rock'        : 'Classic Rock',\n","\n","    'southern hip hop'  : 'Hardcore Hip-Hop & Rock/EDM',\n","    'hard rock'         : 'Hardcore Hip-Hop & Rock/EDM',\n","    'electro house'     : 'Hardcore Hip-Hop & Rock/EDM',\n","    'gangster rap'      : 'Hardcore Hip-Hop & Rock/EDM',\n","\n","    'neo soul'          : 'Soul & Big Room EDM',\n","    'big room'          : 'Soul & Big Room EDM',\n","\n","    'new jack swing'    : 'Retro Swing & Prog House',\n","    'progressive electro house' : 'Retro Swing & Prog House'\n","}\n","\n","df['playlist_genre_grouped'] = df['playlist_subgenre'].map(genre_map).astype('string')\n","\n","if project_config['split_df'] == '1':\n","  test_df['playlist_genre_grouped'] = test_df['playlist_subgenre'].map(genre_map).astype('string')\n","\n","small_cat_cols.append('playlist_genre_grouped')\n","cat_cols.append('playlist_genre_grouped')\n","\n","small_cat_cols.remove('playlist_subgenre')\n","cat_cols.remove('playlist_subgenre')\n","pickle_col(df, col='playlist_subgenre', drop_col=True)"],"metadata":{"id":"90wk52zldiRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyIbgqv4gQiK"},"outputs":[],"source":["# target encoding: replace categories with mean_popularity of each category\n","for col in cat_cols:\n","\n","  if col in df.columns: # maybe I'm just reruning the cell\n","\n","    apply_cat_mean_as_target_encode(\n","        df,\n","        cat_cols=[col],\n","        encode_cols=[f'{col}_mean_popularity'],\n","        target_col='track_popularity',\n","        test_df=test_df)\n","\n","#keep original col for visuaizations later, it will not be used for the train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0k0Pq5ElZmLH"},"outputs":[],"source":["# plot categories mean popularity\n","\n","plot_columns = 3\n","n_plot = len(large_cat_cols) + len(small_cat_cols) + len(cat_cols)\n","plot_rows = n_plot//plot_columns + n_plot%plot_columns\n","\n","fig = plt.figure(figsize=(20,28))\n","plt.subplots_adjust(hspace=1.0)\n","sb.set(font_scale=0.8)\n","\n","for plot_counter, col in enumerate(large_cat_cols, start=1):\n","  plt.subplot(plot_rows, plot_columns, plot_counter)\n","  sb.histplot(x=f\"{col}_mean_popularity\", data=df, kde=True, bins=15)\n","  # pickle_col(df, col=col, drop_col=True)  # we dont need the original col anymore\n","\n","for plot_counter, col in enumerate(small_cat_cols, start=1):\n","    plt.subplot(plot_rows, plot_columns, len(large_cat_cols) + plot_counter)\n","    order = df.groupby(col)[f\"{col}_mean_popularity\"].mean().sort_values(ascending=False).index\n","    sb.barplot(x=col, y=f\"{col}_mean_popularity\", data=df, order=order)\n","    plt.xticks(rotation=85)\n","    #keep orirignal cols for eda visualzations\n","\n","for plot_counter, col in enumerate(small_cat_cols, start=1):\n","    plt.subplot(plot_rows, plot_columns, len(large_cat_cols) + len(small_cat_cols) + plot_counter)\n","    sb.countplot(x=col, data=df)\n","    plt.xticks(rotation=85)\n","    #keep orirignal cols for eda visualzations\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x6oXKYlHpvxo"},"source":["# Continuance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTCQkFpAp4ms"},"outputs":[],"source":["# plot values\n","plot_columns = 3\n","plot_rows = (len(cont_cols)//plot_columns) + len(cont_cols)%plot_columns\n","\n","fig = plt.figure(figsize=(20,15))\n","plt.subplots_adjust(hspace=0.8)\n","sb.set(font_scale=1.2)\n","\n","for plot_counter, col in enumerate(cont_cols, start=1):\n","    plt.subplot(plot_rows, plot_columns, plot_counter)\n","    sb.histplot(x=col, data=df, kde=True)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P1EKeoV39She"},"source":["# Pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcaTZ1859XTf"},"outputs":[],"source":["from google.colab import files\n","import os\n","\n","with open(f\"{project_config['project_path']}{project_config['pickles_directory']}{project_config['pickle1']}\", 'wb') as f:\n","  pickle.dump(df, f)\n","  f.close()\n","\n","if project_config['split_df'] == '1':\n","  with open(f\"{project_config['project_path']}{project_config['pickles_directory']}{project_config['pickle1_test']}\", 'wb') as f:\n","    pickle.dump(test_df, f)\n","    f.close()"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1z_z6aKOseMCo3v_YjFnMhl-K4gXimAMR","authorship_tag":"ABX9TyNa9fcd3eC6wMPmFfTYObiV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}